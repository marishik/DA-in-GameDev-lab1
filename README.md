# АНАЛИЗ ДАННЫХ И ИСКУССТВЕННЫЙ ИНТЕЛЛЕКТ [in GameDev]
Отчет по лабораторной работе #3 выполнила:
- Алексеева Марина Геннадьевна
- РИ210931
Отметка о выполнении заданий (заполняется студентом):

| Задание | Выполнение | Баллы |
| ------ | ------ | ------ |
| Задание 1 | * | 60 |
| Задание 2 | * | 20 |
| Задание 3 | # | 20 |

знак "*" - задание выполнено; знак "#" - задание не выполнено;

Работу проверили:
- к.т.н., доцент Денисов Д.В.
- к.э.н., доцент Панов М.А.
- ст. преп., Фадеев В.О.

[![N|Solid](https://cldup.com/dTxpPi9lDf.thumb.png)](https://nodesource.com/products/nsolid)

[![Build Status](https://travis-ci.org/joemccann/dillinger.svg?branch=master)](https://travis-ci.org/joemccann/dillinger)

Структура отчета

- Данные о работе: название работы, фио, группа, выполненные задания.
- Цель работы.
- Задание 1.
- Код реализации выполнения задания. Визуализация результатов выполнения (если применимо).
- Задание 2.
- Код реализации выполнения задания. Визуализация результатов выполнения (если применимо).
- Задание 3.
- Код реализации выполнения задания. Визуализация результатов выполнения (если применимо).
- Выводы.
- ✨Magic ✨

## Цель работы
Познакомиться с программными средствами для создания системы машинного обучения и ее интеграции в Unity.

## Задание 1
### Реализовать систему машинного обучения в связке Python - Google-Sheets – Unity.

Ход работы: 
1. Создала новый пустой 3D проект на Unity.
2. Скачала папку с ML агентом. Добавила ML Agent и .json – файлы:

![изображение](https://user-images.githubusercontent.com/114138439/204817107-c529cc1e-8ba1-4e3c-ac45-3c82776fb1c1.png)

3. Запустила Anaconda Prompt для возможности запуска команд через консоль. Написала серию команд для создания и активации нового ML-агента и для скачивания необходимых библиотек: mlagents 0.28.0 и torch 1.7.1:

![изображение](https://user-images.githubusercontent.com/114138439/204817227-db791471-3abf-40a7-abc9-7d15c7843d18.png)

4. В сцене создала плоскость, куб и сферу:
![изображение](https://user-images.githubusercontent.com/114138439/204815129-943a572d-29e0-4d53-b4ff-bcc8c44219ae.png)

C# код:
```

```
![изображение](https://user-images.githubusercontent.com/114138439/204817023-3a59a76c-5e5f-4297-abb4-32a047036d8e.png)


5. После добавления файла конфигурации нейронной сети в корень проекта, запустила работу ML-агента:
```
behaviors:
  RollerBall:
    trainer_type: ppo
    hyperparameters:
      batch_size: 10
      buffer_size: 100
      learning_rate: 3.0e-4
      beta: 5.0e-4
      epsilon: 0.2
      lambd: 0.99
      num_epoch: 3
      learning_rate_schedule: linear
    network_settings:
      normalize: false
      hidden_units: 128
      num_layers: 2
    reward_signals:
      extrinsic:
        gamma: 0.99
        strength: 1.0
    max_steps: 500000
    time_horizon: 64
    summary_freq: 10000
```
![изображение](https://user-images.githubusercontent.com/114138439/204817503-598bb244-0561-4fb7-866b-78e79196c368.png)

6. Сделала 3, 9, 27 копий модели «Плоскость-Сфера-Куб», запустила симуляцию сцены и смотрела за результатом обучения модели, чтобы сделать выводы
![изображение](https://user-images.githubusercontent.com/114138439/204823760-8510df9d-c8c1-44c0-95b7-0f41c24df5e2.png)

![изображение](https://user-images.githubusercontent.com/114138439/204818700-77416773-eff0-4df9-a3d5-f71097ce1aa7.png)

![изображение](https://user-images.githubusercontent.com/114138439/204823944-bd6744a3-6308-418c-b19c-7c78d948fa91.png)

Так я поняла, что чем больше количество агентов (копий модели) в Unity, тем модель быстрее обучается, т.е. достигает более высоких показателей точности за меньшее время итераций.


## Задание 2
### Подробно опишите каждую строку файла конфигурации нейронной сети, доступного в папке с файлами проекта по ссылке. Самостоятельно найдите информацию о компонентах Decision Requester, Behavior Parameters, добавленных на сфере.

Код конфигурации нейронной сети:
```
behaviors: /* Описание поведения объекта */
  RollerBall: /* Имя объекта */
    trainer_type: ppo /* Тип обучения с вознаграждением */
    hyperparameters: /* Гиперпараметры */
      batch_size: 10 /* Количество опытов за одну итерацию градиентного спуска */
      buffer_size: 100 /* Размер опыта, необходимый перед обновлением модели политики */
      learning_rate: 3.0e-4 /* Начальная скорость обучения для градиентного спуска */
      beta: 5.0e-4 /* Сила регуляризации энтропии */
      epsilon: 0.2 /* Влияет на быстроту изменения поведения во время обучения */
      lambd: 0.99 /* Лямбда - параметр регуляризации */
      num_epoch: 3 /* Количество проходов через буфер опыта при выполнении оптимизации градиентного спуска */
      learning_rate_schedule: linear /* Параметр изменения скорости обучения с течением времени */
    network_settings: /* Сетевые установки */
      normalize: false /* Параметр, определяющий нормализацию к входным данным */
      hidden_units: 128 /* Количество нейронов в скрытых слоях */
      num_layers: 2 /* Количество скрытых слоёв сети */
    reward_signals: /* Настройки внешних и внутренних сигналов вознаграждения */
      extrinsic: /* Проверка того, что тренировочный прогон включает в себя сигнал вознаграждения */
        gamma: 0.99 /* Коэффициент поощерения (должен быть < 1) */
        strength: 1.0 /* Коэффициент силы, на который умножается вознаграждение */
    max_steps: 500000 /* Максимальное количество повторов симуляции сцены */
    time_horizon: 64 /* Количество шагов опыта, которое нужно собрать для каждого агента, хранящихся в буфере до ввода в модель */
    summary_freq: 10000 /* Количество опыта, которое необходимо собрать перед созданием и отображением статистики тренировок по шагам */
```

## Выводы
В ходе лабораторной работы я познакомилась с программными средствами для создания системы машинного обучения и ее интеграции в Unity.
Игровой баланс - это то свойство игры, которое отвечает за сбалансированность и равновесие, за продуманность различных механик. Баланс в играх заключается в том, что он позволяет игроку почувствовать рост скилла при прохождении игры. Он отвечает за полноценный опыт, полученный при прохождении игры, и важно, чтобы этот опыт был положительным. Баланс должен быть как в сюжете, так и в сложности игры, а создать его и точно распределять игровые механики, подлежащие балансу, помогает машинное обучение. 


| Plugin | README |
| ------ | ------ |
| Dropbox | [plugins/dropbox/README.md][PlDb] |
| GitHub | [plugins/github/README.md][PlGh] |
| Google Drive | [plugins/googledrive/README.md][PlGd] |
| OneDrive | [plugins/onedrive/README.md][PlOd] |
| Medium | [plugins/medium/README.md][PlMe] |
| Google Analytics | [plugins/googleanalytics/README.md][PlGa] |
